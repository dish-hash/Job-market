{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQaHubrhvJuA+hvmd2vbXQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dish-hash/Job-market/blob/main/bde_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRBj53dBKio0",
        "outputId": "dc716fca-53a9-444b-942e-ac4481872198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pyspark 3.5.3\n",
            "Uninstalling pyspark-3.5.3:\n",
            "  Would remove:\n",
            "    /usr/local/bin/beeline\n",
            "    /usr/local/bin/beeline.cmd\n",
            "    /usr/local/bin/docker-image-tool.sh\n",
            "    /usr/local/bin/find-spark-home\n",
            "    /usr/local/bin/find-spark-home.cmd\n",
            "    /usr/local/bin/find_spark_home.py\n",
            "    /usr/local/bin/load-spark-env.cmd\n",
            "    /usr/local/bin/load-spark-env.sh\n",
            "    /usr/local/bin/pyspark\n",
            "    /usr/local/bin/pyspark.cmd\n",
            "    /usr/local/bin/pyspark2.cmd\n",
            "    /usr/local/bin/run-example\n",
            "    /usr/local/bin/run-example.cmd\n",
            "    /usr/local/bin/spark-class\n",
            "    /usr/local/bin/spark-class.cmd\n",
            "    /usr/local/bin/spark-class2.cmd\n",
            "    /usr/local/bin/spark-connect-shell\n",
            "    /usr/local/bin/spark-shell\n",
            "    /usr/local/bin/spark-shell.cmd\n",
            "    /usr/local/bin/spark-shell2.cmd\n",
            "    /usr/local/bin/spark-sql\n",
            "    /usr/local/bin/spark-sql.cmd\n",
            "    /usr/local/bin/spark-sql2.cmd\n",
            "    /usr/local/bin/spark-submit\n",
            "    /usr/local/bin/spark-submit.cmd\n",
            "    /usr/local/bin/spark-submit2.cmd\n",
            "    /usr/local/bin/sparkR\n",
            "    /usr/local/bin/sparkR.cmd\n",
            "    /usr/local/bin/sparkR2.cmd\n",
            "    /usr/local/lib/python3.10/dist-packages/pyspark-3.5.3.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/pyspark/*\n",
            "Proceed (Y/n)? y\n",
            "y\n",
            "  Successfully uninstalled pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwI_ivq9KuMJ",
        "outputId": "2d423d51-9ec3-4bf0-b9d1-60b64dad92ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=632fdff45334902602547cfa0e5682f6715e135541c8cb815f08a65817db0fb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow.fs as fs\n",
        "hdfs = fs.HadoopFileSystem('hdfs://namenode_host:9000')\n",
        "hdfs_path = '/Users/Jin boy/Documents/job market dataset/Naukri_Data_Scientist_and_Data_Analytics_Jobs_Data.parquet'\n",
        "\n",
        "# Read the Parquet dataset from HDFS\n",
        "dataset = pq.ParquetDataset(hdfs_path, filesystem=hdfs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "hjVAaVAGK9Ag",
        "outputId": "88fb0295-1bf1-4fd3-d3b7-24c456f03b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7aa4298e-8112-48c9-821c-58e7759f3f09\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7aa4298e-8112-48c9-821c-58e7759f3f09\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Naukri_Data_Scientist_and_Data_Analytics_Jobs_Data.csv to Naukri_Data_Scientist_and_Data_Analytics_Jobs_Data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PySpark\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tC1CCBHaMMaD",
        "outputId": "11e2a68f-a60b-4bfc-9dff-c51bfa017b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:3 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:4 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,452 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,696 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,353 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,397 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,241 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,163 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,611 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,456 kB]\n",
            "Fetched 25.8 MB in 4s (5,921 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\"\n",
        "\n",
        "# Initialize PySpark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Import and start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Check if Spark is running\n",
        "spark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "aC3eRLkDNC9k",
        "outputId": "db29f8b5-4f06-432c-b082-e8eb17b33a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x799d80d861d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://97ddbfa70adb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload CSV file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Define the file path after uploading\n",
        "file_path = \"/content/Naukri_Data_Scientist_and_Data_Analytics_Jobs_Data.csv\"\n",
        "\n",
        "# Read the CSV file into a PySpark DataFrame\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the first few rows\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "rLSvoWRvNKEv",
        "outputId": "5aaf0667-d75f-4d71-ec9c-e08e971fc9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-76b40189-e27e-485f-8b74-5a33c757192d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-76b40189-e27e-485f-8b74-5a33c757192d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Naukri_Data_Scientist_and_Data_Analytics_Jobs_Data.csv to Naukri_Data_Scientist_and_Data_Analytics_Jobs_Data (1).csv\n",
            "+--------------------+--------------------+-------------------+-------------+--------------------+--------------------+\n",
            "|          Job Titles|       Company Names|Experience Required|      Package|           Locations|              Skills|\n",
            "+--------------------+--------------------+-------------------+-------------+--------------------+--------------------+\n",
            "|Manager - Digital...|                Resy|            4-8 Yrs|Not disclosed|    Gurgaon/Gurugram|Product managemen...|\n",
            "|Data Science Doma...|            Coursera|           7-11 Yrs|Not disclosed|Kolkata, Mumbai, ...|Computer scienceC...|\n",
            "|GN - Strategy - M...|           Accenture|            1-3 Yrs|Not disclosed|Mumbai, Hyderabad...|Change management...|\n",
            "|Data Science Manager|Foreign IT Consul...|            2-6 Yrs|Not disclosed|    Gurgaon/Gurugram|Operations resear...|\n",
            "|Data Science Manager|Foreign IT Consul...|            3-6 Yrs|Not disclosed|               Noida|Data analysisEDCR...|\n",
            "+--------------------+--------------------+-------------------+-------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import last, col\n",
        "\n",
        "# Define a window specification to order by 'Job Titles' (or another suitable column)\n",
        "window_spec = Window.orderBy(\"Job Titles\").rowsBetween(Window.unboundedPreceding, 0)\n",
        "\n",
        "# Forward fill the 'Experience Required' column\n",
        "df = df.withColumn(\"Experience Required\", last(col(\"Experience Required\"), ignorenulls=True).over(window_spec))\n",
        "\n",
        "# Forward fill the 'Company Names' column\n",
        "df = df.withColumn(\"Company Names\", last(col(\"Company Names\"), ignorenulls=True).over(window_spec))\n",
        "\n",
        "# Show the DataFrame to verify forward filling\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y5D7H6yPEay",
        "outputId": "d2dbffc7-238e-4291-cd9c-1f24752d2b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-------------------+-------------+--------------------+--------------------+\n",
            "|Job Titles|Company Names|Experience Required|      Package|           Locations|              Skills|\n",
            "+----------+-------------+-------------------+-------------+--------------------+--------------------+\n",
            "|      null|         null|           6-10 Yrs|Not disclosed|Gurgaon/ Gurugram...|Advanced Statisti...|\n",
            "|      null|         null|           8-10 Yrs|Not disclosed|Hyderabad/Secunde...|Statistical progr...|\n",
            "|      null|         null|          12-15 Yrs|Not disclosed|Hybrid - Noida, U...|NLPData SciencePr...|\n",
            "|      null|         null|            7-9 Yrs|Not disclosed|             Chennai|SQL queriesData a...|\n",
            "|      null|         null|            3-7 Yrs|10-20 Lacs PA|Hybrid - Gurgaon/...|SQLBusiness Intel...|\n",
            "+----------+-------------+-------------------+-------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate rows\n",
        "df = df.dropDuplicates()\n",
        "\n",
        "# Show the count of rows after removing duplicates\n",
        "print(\"Number of rows after removing duplicates:\", df.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWME022aSQcu",
        "outputId": "3310c9f8-8cc0-4ce9-e6b2-5f88ced8b723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows after removing duplicates: 19646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexer = StringIndexer(inputCol=\"Job Titles\", outputCol=\"New_Job_Title_Index\", handleInvalid=\"skip\")\n",
        "df = indexer.fit(df).transform(df)\n"
      ],
      "metadata": {
        "id": "p4U2UsI0ST8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(\"Job_Title_Index\")\n",
        "indexer = StringIndexer(inputCol=\"Job Titles\", outputCol=\"Job_Title_Index\", handleInvalid=\"skip\")\n",
        "df = indexer.fit(df).transform(df)\n"
      ],
      "metadata": {
        "id": "SyyIuymkSsaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the existing column if it exists\n",
        "if \"Job_Title_Index\" in df.columns:\n",
        "    df = df.drop(\"Job_Title_Index\")\n",
        "\n",
        "# Create and apply the StringIndexer\n",
        "indexer = StringIndexer(inputCol=\"Job Titles\", outputCol=\"Job_Title_Index\", handleInvalid=\"skip\")\n",
        "df = indexer.fit(df).transform(df)\n"
      ],
      "metadata": {
        "id": "rFemqOMdSvvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill nulls in 'Experience Required' with a default value, e.g., \"0 years\"\n",
        "df = df.fillna({'Experience Required': '0 years'})\n"
      ],
      "metadata": {
        "id": "A_kmJdUFSylB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill nulls in the 'Package' column\n",
        "df = df.fillna({'Package': 'Not Specified'})\n"
      ],
      "metadata": {
        "id": "lbZ4DwDrTDAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill nulls in multiple columns\n",
        "df = df.fillna({\n",
        "    'Experience Required': '0 years',\n",
        "    'Package': 'Not Specified',\n",
        "    'Locations': 'Unknown'\n",
        "})\n"
      ],
      "metadata": {
        "id": "2ejejvCITGLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, when, col\n"
      ],
      "metadata": {
        "id": "6dwL-jhBTIVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, when, col\n",
        "\n",
        "# Count null values in each column\n",
        "null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "null_counts.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5TrBowaTRFl",
        "outputId": "0c9d29fc-f8d0-4e41-d9c1-f3ac3487c916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+-------------------+-------+---------+------+------------------+-------------------+---------------+\n",
            "|Job Titles|Company Names|Experience Required|Package|Locations|Skills|Company_Name_Index|New_Job_Title_Index|Job_Title_Index|\n",
            "+----------+-------------+-------------------+-------+---------+------+------------------+-------------------+---------------+\n",
            "|         0|            0|                  0|      0|        0|     0|                 0|                  0|              0|\n",
            "+----------+-------------+-------------------+-------+---------+------+------------------+-------------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame creation (replace this with your actual DataFrame)\n",
        "data = [(\"A\",), (\"B\",), (\"C\",), (\"A\",), (\"B\",)]\n",
        "df = spark.createDataFrame(data, [\"Package\"])\n",
        "\n",
        "# Step 1: Index the 'Package' column\n",
        "indexer = StringIndexer(inputCol=\"Package\", outputCol=\"Package_Index\")\n",
        "df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Step 2: Assemble the indexed column into a vector\n",
        "assembler = VectorAssembler(inputCols=[\"Package_Index\"], outputCol=\"Package_Feature\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Show the transformed DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxI79Bn_Td9w",
        "outputId": "6f1f736e-3c4d-4c7d-da3f-c2d6de3fac1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+---------------+\n",
            "|Package|Package_Index|Package_Feature|\n",
            "+-------+-------------+---------------+\n",
            "|      A|          0.0|          [0.0]|\n",
            "|      B|          1.0|          [1.0]|\n",
            "|      C|          2.0|          [2.0]|\n",
            "|      A|          0.0|          [0.0]|\n",
            "|      B|          1.0|          [1.0]|\n",
            "+-------+-------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets (80% training, 20% testing)\n",
        "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(\"Training Dataset Count:\", train_df.count())\n",
        "print(\"Test Dataset Count:\", test_df.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufXcJ_WzVZf7",
        "outputId": "28d2406b-ba38-4831-d1c7-52a12765fbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Count: 4\n",
            "Test Dataset Count: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7mtCiTjVbQ3",
        "outputId": "27c48388-8f97-4073-d143-ffbf6a2ecf0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Package: string (nullable = true)\n",
            " |-- Package_Index: double (nullable = false)\n",
            " |-- Package_Feature: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "from google.colab import files\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
        "\n",
        "# Your existing DataFrame operations\n",
        "# df = ... (your DataFrame after preprocessing)\n",
        "\n",
        "# Check the schema of the DataFrame\n",
        "df.printSchema()\n",
        "\n",
        "# Define a UDF to convert array to string\n",
        "array_to_string_udf = udf(lambda arr: \",\".join(map(str, arr)) if arr is not None else None, StringType())\n",
        "\n",
        "# Apply the UDF to convert the array column to string\n",
        "df = df.withColumn(\"Package_Feature\", array_to_string_udf(col(\"Package_Feature\")))\n",
        "\n",
        "# Save the cleaned DataFrame to a new CSV file\n",
        "output_file_path = \"/content/cleaned_data\"\n",
        "\n",
        "# Remove previous directory if it exists (this won't cause an error if it doesn't exist)\n",
        "!rm -r /content/cleaned_data\n",
        "\n",
        "# Write the DataFrame to CSV\n",
        "df.write.option(\"header\", \"true\").csv(output_file_path)\n",
        "\n",
        "# Confirm the files are saved\n",
        "print(f\"Cleaned DataFrame saved to {output_file_path}\")\n",
        "\n",
        "# Optional: Zip and download the CSV files\n",
        "!zip -r cleaned_data.zip /content/cleaned_data\n",
        "files.download('cleaned_data.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "HTx5Uo0XW6be",
        "outputId": "57d098e3-09a9-45c8-d3ac-557191f5606a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Package: string (nullable = true)\n",
            " |-- Package_Index: double (nullable = false)\n",
            " |-- Package_Feature: array (nullable = true)\n",
            " |    |-- element: double (containsNull = true)\n",
            "\n",
            "rm: cannot remove '/content/cleaned_data': No such file or directory\n",
            "Cleaned DataFrame saved to /content/cleaned_data\n",
            "  adding: content/cleaned_data/ (stored 0%)\n",
            "  adding: content/cleaned_data/_SUCCESS (stored 0%)\n",
            "  adding: content/cleaned_data/.part-00000-def9c334-1df9-49de-87f0-58520b3e1278-c000.csv.crc (stored 0%)\n",
            "  adding: content/cleaned_data/part-00001-def9c334-1df9-49de-87f0-58520b3e1278-c000.csv (deflated 26%)\n",
            "  adding: content/cleaned_data/.part-00001-def9c334-1df9-49de-87f0-58520b3e1278-c000.csv.crc (stored 0%)\n",
            "  adding: content/cleaned_data/part-00000-def9c334-1df9-49de-87f0-58520b3e1278-c000.csv (deflated 26%)\n",
            "  adding: content/cleaned_data/._SUCCESS.crc (stored 0%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a7fd07f6-d570-415e-84f4-f81b894ddbe1\", \"cleaned_data.zip\", 1847)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}